<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="VAPA: Realistic Video-based Human Avatar by Personalized Adaptation of Video Diffusion Model">
  <meta property="og:title" content="VAPA: Realistic Video-based Human Avatar by Personalized Adaptation of Video Diffusion Model"/>
  <meta property="og:description" content="VAPA: Realistic Video-based Human Avatar by Personalized Adaptation of Video Diffusion Model"/>
  <meta property="og:url" content="https://carlyx.github.io/DPE/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/carousel1.jpg" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="VAPA: Realistic Video-based Human Avatar by Personalized Adaptation of Video Diffusion Model">
  <meta name="twitter:description" content="VAPA: Realistic Video-based Human Avatar by Personalized Adaptation of Video Diffusion Model">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/carousel1.jpg">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>VAPA</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h3 class="title is-1 publication-title">
              VAPA: Realistic Video-based Human Avatar by Personalized Adaptation of Video Diffusion Model
            </h3>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="http://carlyx.github.io/" target="_blank">Youxin Pang</a><sup>1,2,3</sup>
              </span>
                <span class="author-block">
                  <a href="https://yzhang2016.github.io/" target="_blank">Yong Zhang</a><sup>3</sup>
                </span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?hl=zh-CN&user=PMR9cooAAAAJ" target="_blank">Weize Quan</a><sup>1,2</sup>
                  </span>
                  <span class="author-block">
                    <a href="https://sites.google.com/site/yanbofan0124/" target="_blank">Yanbo Fan</a><sup>3</sup>
                  </span>
                  <span class="author-block">
                    <a href="https://vinthony.github.io/academic/" target="_blank">Xiaodong Cun</a><sup>3</sup>
                  </span>
                  <br>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=zh-CN" target="_blank">Ying Shan</a><sup>3</sup>
                  </span>
                  <span class="author-block">
                    <a href="https://sites.google.com/site/yandongming/" target="_blank">Dong-ming Yan</a><sup>1,2</sup>
                  </span>
                  </div>
                  <br>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <sup>1</sup> MAIS & NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                      <sup>2</sup> School of Artificial Intelligence, University of Chinese Academy of Sciences &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                      <sup>3</sup> Tencent AI Lab, ShenZhen, China
                      <br>CVPR 2023</span>
                  </div>
                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2301.06281" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Carlyx/DPE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span>ðŸ¤— Demo (Hugging Face Space)</span>
                </a>
              </span>

              <span class="link-block">
                  <a href="" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span>ðŸ§¿ Demo (Colab)</span>
                </a>
              </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            One-shot video-driven talking face generation aims at producing a synthetic talking video by  transferring the facial motion from a video to an arbitrary portrait image. 
            Head pose and facial expression are always entangled in facial motion and transferred simultaneously.
            However, the entanglement sets up a barrier for these methods to be used in video portrait editing directly, where it may require to modify the expression only while maintaining the pose unchanged. 
            One challenge of decoupling pose and expression is the lack of paired data, such as the same pose but different expressions.
            Only a few methods attempt to tackle this challenge with the feat of 3D Morphable Models (3DMMs) for explicit disentanglement. 
            But 3DMMs are not accurate enough to capture facial details due to the limited number of Blendshapes, which has side effects on motion transfer. 
            In this paper, we introduce a novel self-supervised disentanglement framework to decouple pose and expression without 3DMMs and paired data, which consists of a motion editing module, a pose generator, and an expression generator. 
            The editing module projects faces into a latent space where pose motion and expression motion can be disentangled, and the pose or expression transfer can be performed in the latent space conveniently via addition. 
            The two generators render the modified latent codes to images, respectively. 
            Moreover, to guarantee the disentanglement, we propose a bidirectional cyclic training strategy with well-designed constraints.  
            Evaluations demonstrate our method can control pose or expression independently and be used for general video editing.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/vinthony/project-page-template">modification version</a> of <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> from <a href="https://github.com/vinthony">vinthony</a>.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br>
            Some source videos in this website are selected from <a href="https://www.colossyan.com/">here</a>
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
