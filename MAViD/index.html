<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="MAViD: A Multimodal Framework for Audio-Visual Dialogue Understanding and Generation">
  <meta property="og:title" content="MAViD: A Multimodal Framework for Audio-Visual Dialogue Understanding and Generation"/>
  <meta property="og:description" content="MAViD: A Multimodal Framework for Audio-Visual Dialogue Understanding and Generation"/>
  <meta property="og:url" content="https://carlyx.github.io/MAViD/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/carousel1.jpg" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="MAViD: A Multimodal Framework for Audio-Visual Dialogue Understanding and Generation">
  <meta name="twitter:description" content="MAViD: A Multimodal Framework for Audio-Visual Dialogue Understanding and Generation">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/carousel1.jpg">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>MAViD</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h3 class="title is-1 publication-title">
              MAViD: A Multimodal Framework for Audio-Visual Dialogue Understanding and Generation
            </h3>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="http://carlyx.github.io/" target="_blank">Youxin Pang</a><sup>1,2,*</sup>
              </span>
              <span class="author-block">
                <a href="" target="_blank">Jiajun Liu</a><sup>2,*</sup>
              </span>
              <span class="author-block">
                <a href="" target="_blank">Lingfeng Tan</a><sup>2,*</sup>
              </span>
                <span class="author-block">
                  <a href="https://yzhang2016.github.io/" target="_blank">Yong Zhang</a><sup>2</sup>
                </span>
                  <span class="author-block">
                    <a href="" target="_blank">Feng Gao</a><sup>2</sup>
                  </span>
                  <br>
                  <span class="author-block">
                    <a href="" target="_blank">Xiang Deng</a><sup>1,2</sup>
                  </span>
                  <span class="author-block">
                    <a href="" target="_blank">Zhuoliang Kang</a><sup>2</sup>
                  </span>
                  <span class="author-block">
                    <a href="" target="_blank">Xiaoming Wei</a><sup>2</sup>
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=ogXIdlYAAAAJ&hl=zh-CN" target="_blank">Yebin Liu</a><sup>1</sup>
                  </span>
                  </div>
                  <br>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <sup>1</sup> Tsinghua University &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                      <sup>2</sup> Meituan &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                      <!-- <br>CVPR 2023</span> -->
                  </div>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <sup>*</sup> Equal Contribution &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                      <!-- <br>CVPR 2023</span> -->
                  </div>
                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a href="" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Carlyx/MAViD" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span>ðŸ¤— Demo (Hugging Face Space)</span>
                </a>
              </span>

              <span class="link-block">
                  <a href="" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span>ðŸ§¿ Demo (Colab)</span>
                </a>
              </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We propose MAViD, a novel Multimodal framework for Audio-Visual Dialogue understanding and generation.Existing approaches primarily focus on non-interactive systems and are limited to producing constrained and unnatural human speech.The primary challenge of this task lies in effectively integrating understanding and generation capabilities, as well as achieving seamless multimodal audio-video fusion.To solve these problems, we propose a Conductorâ€“Creator architecture that divides the dialogue system into two primary components.The Conductor is tasked with understanding, reasoning, and generating instructions by breaking them down into motion and speech components, thereby enabling fine-grained control over interactions.The Creator then delivers interactive responses based on these instructions.Furthermore, to address the difficulty of generating long videos with consistent identity, timbre, and tone using dual DiT structures, the Creator adopts a structure that combines autoregressive (AR) and diffusion models. The AR model is responsible for audio generation, while the diffusion model ensures high-quality video generation.Additionally, we propose a novel fusion module to enhance connections between contextually consecutive clips and modalities, enabling synchronized long-duration audio-visual content generation.Extensive experiments demonstrate that our framework can generate vivid and contextually coherent long-duration dialogue interactions and accurately interpret users' multimodal queries.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/vinthony/project-page-template">modification version</a> of <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> from <a href="https://github.com/vinthony">vinthony</a>.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br>
            Some source videos in this website are selected from <a href="https://www.colossyan.com/">here</a>
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
